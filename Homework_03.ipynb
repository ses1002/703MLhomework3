{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46913eb8-92a6-4118-b860-869d4a889226",
   "metadata": {},
   "source": [
    "# Homework 03: Learning Curves and Training Workflow\n",
    "\n",
    "## Due: Midnight on September 21 (with 2-hour grace period)\n",
    "\n",
    "**Points:** 85\n",
    "\n",
    "In this assignment, you will learn how to design, train, and evaluate neural networks by systematically exploring key design choices. Your focus will be on developing an effective **training workflow** — using learning curves and validation metrics to guide your decisions.\n",
    "\n",
    "We'll use the **Forest Cover Type (Covertype) dataset,** which has ~581k tabular records with 54 cartographic/topographic features (elevation, aspect, slope, soil and wilderness indicators) used to predict one of seven tree cover types in Colorado’s Roosevelt National Forest. It’s a large, mildly imbalanced multi-class benchmark commonly used to compare classical ML and deep learning on tabular data.\n",
    "\n",
    "We will start with a **baseline model** (two hidden layers of sizes 64 and 32), and gradually introduce and tune different hyperparameters. Each of the first five problems considers  different hyperparameter choices, and the last problem is your chance to use what you have learned to design your best model:\n",
    "\n",
    "1. **Activation function** – Compare ReLU, sigmoid, and tanh to see which provides the best accuracy.\n",
    "2. **Learning rate** – Explore a range of learning rates and identify which balances convergence speed and stability.\n",
    "3. **Dropout** – Investigate how different dropout rates reduce overfitting and where they are most effective.\n",
    "4. **L2 regularization** – Experiment with weight penalties to encourage simpler models and avoid memorization.\n",
    "5. **Dropout + L2** – Combine both regularization techniques and study their interaction.\n",
    "6. **Best model design** – Use all your insights to build and train your strongest model, with the option to try **learning rate scheduling** for further improvement.\n",
    "\n",
    "Throughout, you will use **early stopping** to select the model at the epoch of **minimum validation loss**, and you will report the **validation accuracy** of that selected model as the primary measure of performance.\n",
    "\n",
    "By the end of this homework, you will not only understand how different hyperparameters affect training and generalization, but also gain hands-on practice in building a disciplined workflow for model development.\n",
    "\n",
    "There are 10 graded problems, worth 8 points each, with 5 points for free if you complete the homework. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "647c362e-70fa-4713-a7e5-b9172b3c4a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful imports\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import time\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Input,Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# utility code\n",
    "\n",
    "random_seed = 42\n",
    "\n",
    "def format_hms(seconds):\n",
    "    return time.strftime(\"%H:%M:%S\", time.gmtime(seconds))\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppresses INFO and WARNING messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44fc0cdc-c99e-4983-8526-09ff935ea698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to plot learning curves and keep track of all results\n",
    "\n",
    "# Call `print_results()` to see listing of all results logged so far\n",
    "\n",
    "\n",
    "def plot_learning_curves(hist, title, verbose=True):\n",
    "    \n",
    "    val_losses = hist.history['val_loss']\n",
    "    min_val_loss = min(val_losses)\n",
    "    min_val_epoch = val_losses.index(min_val_loss)\n",
    "    val_acc_at_min_loss = hist.history['val_accuracy'][min_val_epoch]\n",
    "\n",
    "    epochs = range(1, len(val_losses) + 1)  # epoch numbers starting at 1\n",
    "\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(8, 8), sharex=True)\n",
    "\n",
    "    # --- Loss Plot ---\n",
    "    axs[0].plot(epochs, hist.history['loss'], label='train loss')\n",
    "    axs[0].plot(epochs, hist.history['val_loss'], label='val loss')\n",
    "    axs[0].scatter(min_val_epoch + 1, min_val_loss, color='red', marker='x', s=50, label='min val loss')\n",
    "    axs[0].set_title(f'{title} - Categorical Cross-Entropy Loss')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "\n",
    "    # --- Accuracy Plot ---\n",
    "    axs[1].plot(epochs, hist.history['accuracy'], label='train acc')\n",
    "    axs[1].plot(epochs, hist.history['val_accuracy'], label='val acc')\n",
    "    axs[1].scatter(min_val_epoch + 1, val_acc_at_min_loss, color='red', marker='x', s=50, label='acc @ min val loss')\n",
    "    axs[1].set_title(f'{title} - Accuracy')\n",
    "    axs[1].set_xlabel('Epoch')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "    axs[1].set_ylim(0, 1.05)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Final Training Loss:            {hist.history['loss'][-1]:.4f}\")\n",
    "        print(f\"Final Training Accuracy:        {hist.history['accuracy'][-1]:.4f}\")\n",
    "        print(f\"Final Validation Loss:          {hist.history['val_loss'][-1]:.4f}\")\n",
    "        print(f\"Final Validation Accuracy:      {hist.history['val_accuracy'][-1]:.4f}\")\n",
    "        print(f\"Minimum Validation Loss:        {min_val_loss:.4f} (Epoch {min_val_epoch + 1})\")\n",
    "        print(f\"Validation Accuracy @ Min Loss: {val_acc_at_min_loss:.4f}\")\n",
    "\n",
    "    results[title] = (val_acc_at_min_loss,{min_val_epoch + 1})\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150efd62-0b3d-43f9-99c5-5e6ffd2a7775",
   "metadata": {},
   "source": [
    "**The plotting function will record the validation accuracy for each experiment, using the plot title as key. The next function will print these out (see the last cell in the notebook).**\n",
    "\n",
    "\n",
    "In order to see all results, you must give a different plot title to each experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ad4815c-54bc-4bcb-b275-ac96dda6ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results():\n",
    "    for title, (acc, ep) in sorted(results.items(), \n",
    "                                   key=lambda kv: kv[1][0],   # kv[1] is (acc, epoch); [0] is acc\n",
    "                                   reverse=True\n",
    "                                  ):\n",
    "        print(f\"{title:<40}\\t{acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdba1b4-f572-4b8b-a2eb-d5fd5b253d94",
   "metadata": {},
   "source": [
    "### Wrapper to train, display results, and run test set\n",
    "\n",
    "We assume multi-class classification, and allow setting various parameters for training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1639012c-346c-4ac6-80fe-0e4a05155e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses globals X_train,y_train,X_val,y_val\n",
    "\n",
    "def train_and_test(model, \n",
    "                   epochs        = 500,                   # Just needs to be bigger than early stop point\n",
    "                   lr_schedule   = 0.001,                 # Adam default / 10 seems to work well for this dataset\n",
    "                   optimizer     = \"Adam\",\n",
    "                   title         = \"Learning Curves\",\n",
    "                   batch_size    = 64,                     # experiments confirmed this was optimal with other parameters at default\n",
    "                   use_early_stopping = True,\n",
    "                   patience      = 10,                                       \n",
    "                   min_delta     = 0.0001,                 \n",
    "                   callbacks     = [],                     # for extra callbacks other than early stopping\n",
    "                   verbose       = 0,\n",
    "                   return_history = False\n",
    "                  ):\n",
    "\n",
    "    print(f\"\\n{title}\\n\")\n",
    "\n",
    "\n",
    "    if optimizer == \"Adam\":\n",
    "        opt = Adam(learning_rate=lr_schedule) \n",
    "    else:\n",
    "        opt = optimizer\n",
    "    \n",
    "    #Compiling the model\n",
    "    model.compile(optimizer=opt, \n",
    "                  loss=\"sparse_categorical_crossentropy\", \n",
    "                  metrics=[\"accuracy\"]\n",
    "                 )\n",
    "\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=patience,\n",
    "        min_delta=min_delta,\n",
    "        restore_best_weights=True,               # this will mean that the model which produced the smallest validation loss will be returned\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "\n",
    "    if use_early_stopping:\n",
    "        cbs=[early_stop] + callbacks\n",
    "    else:\n",
    "        cbs=callbacks\n",
    "\n",
    "    # start timer\n",
    "    start = time.time()\n",
    "    \n",
    "    # Fit the model with early stopping\n",
    "    history = model.fit(X_train, y_train,\n",
    "                        epochs=epochs,\n",
    "                        batch_size=batch_size,\n",
    "                        validation_data=(X_val, y_val),       # must use stratified validation set\n",
    "                        callbacks=cbs,\n",
    "                        verbose=verbose\n",
    "                       )\n",
    "\n",
    "    if use_early_stopping:\n",
    "        best_epoch = early_stop.best_epoch\n",
    "        best_acc   = history.history['val_accuracy'][best_epoch]\n",
    "    else:\n",
    "        best_epoch = np.argmax(history.history['val_accuracy'])\n",
    "        best_acc   = history.history['val_accuracy'][best_epoch]\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_learning_curves(history, title=title)\n",
    "\n",
    "    # Evaluate on test data\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    print(f\"\\nValidation-Test Gap (accuracy): {abs(best_acc - test_accuracy):.6f}\")\n",
    "    \n",
    "    # Record end time and print execution time\n",
    "    end = time.time()\n",
    "    print(f\"\\nExecution Time: \" + format_hms(end-start))\n",
    "\n",
    "    if return_history:\n",
    "        return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02915a-92e8-4079-afba-7ae6229cf2f0",
   "metadata": {},
   "source": [
    "### Load the dataset and extract a stratified subset\n",
    "\n",
    "This datasest is rather large (581,012 samples) and unbalanced, but for the purposes of this homework, we use a much smaller set, and select samples so that it is balanced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e526b41-6c2e-45dd-8b26-cc72d4da26df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full dataset shape: (581012, 54)\n",
      "balanced subset shape: (7000, 54) class counts: {2: 1000, 0: 1000, 4: 1000, 5: 1000, 1: 1000, 3: 1000, 6: 1000}\n",
      "shapes: X_train (4200, 54) X_val (1400, 54) X_test (1400, 54)\n",
      "train: total=4200, per-class={2: 600, 1: 600, 0: 600, 3: 600, 5: 600, 4: 600, 6: 600}\n",
      "val  : total=1400, per-class={1: 200, 6: 200, 5: 200, 4: 200, 3: 200, 2: 200, 0: 200}\n",
      "test : total=1400, per-class={0: 200, 6: 200, 2: 200, 3: 200, 4: 200, 5: 200, 1: 200}\n",
      "class labels: [0 1 2 3 4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# complete cell: load, balance, split into X_train/y_train/x_val/y_val/X_test/y_test, and standardize\n",
    "from collections import Counter\n",
    "from sklearn.datasets import fetch_covtype\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# 1) load\n",
    "x, y = fetch_covtype(return_X_y=True)  # y in {1..7}\n",
    "print(\"full dataset shape:\", x.shape)\n",
    "\n",
    "# 2) build a perfectly balanced subset across 7 classes (no replacement)\n",
    "classes, counts = np.unique(y, return_counts=True)\n",
    "# min_count = counts.min()  # size of rarest class                         # You can modify this parameter to increase the size of the dataset, but above\n",
    "min_count = 1000                                                           # counts.min() you'll produce an unbalanced set. \n",
    "\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "idx_list = []\n",
    "for c in classes:\n",
    "    c_idx = np.where(y == c)[0]\n",
    "    chosen = rng.choice(c_idx, size=min_count, replace=False)\n",
    "    idx_list.append(chosen)\n",
    "\n",
    "idx_bal = np.concatenate(idx_list)\n",
    "rng.shuffle(idx_bal)\n",
    "\n",
    "X_sub = x[idx_bal]\n",
    "y_sub = y[idx_bal] - 1  # relabel to {0..6} for keras\n",
    "print(\"balanced subset shape:\", X_sub.shape, \"class counts:\", dict(Counter(y_sub)))\n",
    "\n",
    "# 3) stratified 60/20/20 split (train/val/test)\n",
    "test_size = 0.20\n",
    "val_size = 0.20  # of the whole dataset\n",
    "\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X_sub, y_sub, test_size=test_size, random_state=random_seed, stratify=y_sub\n",
    ")\n",
    "val_size_rel = val_size / (1.0 - test_size)  # e.g., 0.20 / 0.80 = 0.25\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=val_size_rel, random_state=random_seed, stratify=y_trainval\n",
    ")\n",
    "\n",
    "# 4) standardize using train-only stats (float32 for tensorflow friendliness)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train).astype(np.float32)\n",
    "X_val   = scaler.transform(X_val).astype(np.float32)\n",
    "X_test  = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "# 5) quick sanity checks\n",
    "def show_counts(name, y_arr):\n",
    "    c = Counter(y_arr)\n",
    "    total = sum(c.values())\n",
    "    print(f\"{name}: total={total}, per-class={dict(c)}\")\n",
    "\n",
    "print(\"shapes:\", \"X_train\", X_train.shape, \"X_val\", X_val.shape, \"X_test\", X_test.shape)\n",
    "show_counts(\"train\", y_train)\n",
    "show_counts(\"val  \", y_val)\n",
    "show_counts(\"test \", y_test)\n",
    "\n",
    "# you now have: X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Looks like integer encoded multi-class, let's check and define the global n_classes\n",
    "\n",
    "labels = np.unique(y_train)\n",
    "\n",
    "n_classes = len(labels)\n",
    "\n",
    "print(\"class labels:\",labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffebb52-5b6e-4130-bfc3-416a36d9b39b",
   "metadata": {},
   "source": [
    "## Prelude: Defining a model builder\n",
    "\n",
    "In order to facilitate our experimentation, we'll write a function which builds models according to specifications:\n",
    "\n",
    "- How many layers\n",
    "- How wide each layer is\n",
    "- How much dropout in each layer\n",
    "- How much L2 Regularization in each layer\n",
    "\n",
    "This is a fairly standard practice in ML, since the structure of simple models is fairly predictable and can be specified by a few hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7a66816-a103-4477-9693-3936745dbcf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This function will build a multi-class classifier with dropout and L2 regularization.\n",
    "# You must specify the number of input features, the number of classes, and a list of layer hyperparameters\n",
    "# in the form  [ ...., (width, activation function, L2 lambda, dropout rate), .... ]\n",
    "\n",
    "# Note that when adding dropout, this appears as a separate layer, but it has no parameters to be trained. \n",
    "\n",
    "def build_model(n_inputs,layer_list,n_classes):\n",
    "    layers = [ Input(shape=(n_inputs,)) ]\n",
    "    for (width,act,l2_lambda,dropout_rate) in layer_list:\n",
    "        layers.append( Dense(width, activation=act, kernel_regularizer=regularizers.l2(l2_lambda)) )\n",
    "        if dropout_rate > 0:\n",
    "            layers.append( Dropout(dropout_rate) )\n",
    "    layers.append( Dense(n_classes, activation='softmax') )\n",
    "    return models.Sequential( layers )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054cdb1-c764-4d94-9da4-13e138cd7fb4",
   "metadata": {},
   "source": [
    "**Example: To build the following model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "973edccc-a057-402b-be00-3d53e0c8e97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,520</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">231</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m3,520\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m231\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,831</span> (22.78 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,831\u001b[0m (22.78 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,831</span> (22.78 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,831\u001b[0m (22.78 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = models.Sequential(\n",
    "   [\n",
    "    Input(shape=(X_train.shape[1],)),                              \n",
    "    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0)),              # 0.0 means no regularization applied; no dropout, so no Dropout layer necessary\n",
    "    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    Dropout(0.3),\n",
    "    Dense(n_classes, activation='softmax')\n",
    "   ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275f8700-34e8-44c8-a05e-a18c2d331e51",
   "metadata": {},
   "source": [
    "**We call `build_model` as shown here:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "733bc4a8-3480-4a21-aec3-91404d795035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,520</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">231</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m3,520\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │           \u001b[38;5;34m231\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,831</span> (22.78 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,831\u001b[0m (22.78 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,831</span> (22.78 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,831\u001b[0m (22.78 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "build_model(X_train.shape[1], [ (64,'relu',0.0,0.0), (32,'relu',0.001,0.3)], n_classes).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efc2026-137c-4241-bd52-65b0abaec4ca",
   "metadata": {},
   "source": [
    "-------------------\n",
    "\n",
    "\n",
    "\n",
    "### Baseline Model Architecture\n",
    "\n",
    "**Problems 1–5 will use the following baseline model structure,** implemented with the provided `build_model` function and trained using `train_and_test`:\n",
    "\n",
    "```\n",
    "input → 64 → 32 → output\n",
    "```\n",
    "\n",
    "* Two hidden layers of widths 64 and 32.\n",
    "* Activation function, dropout rate, and L2 regularization term (λ) will vary as specified in each problem.\n",
    "* **Early stopping** is always applied to select the model at the epoch of **minimum validation loss**.\n",
    "* We will report the **validation accuracy** of the selected model as the primary metric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb7b1c9-30f0-42ef-9785-4254ceee89bb",
   "metadata": {},
   "source": [
    "### Problem One: Which Activation Function?\n",
    "\n",
    "In this problem, you will train the **baseline neural network** and investigate which activation function produces the best performance. The model you create will be the one saved by **early stopping** — that is, the epoch where validation loss is minimized.\n",
    "\n",
    "**Steps to follow:**\n",
    "\n",
    "* Use the provided functions `train_and_test` and `build_model` to create a model named **`model_baseline`**.\n",
    "* Train and evaluate this model using each of the following activation functions in the hidden layers:\n",
    "\n",
    "  * `relu`\n",
    "  * `sigmoid`\n",
    "  * `tanh`\n",
    "* Identify which activation function produces the **best validation accuracy** at the epoch of **minimum validation loss**.\n",
    "* Answer the graded questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c480aa5a-21cc-4b50-a49f-87dcb3eaa9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Add as many cells as you need. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7599e2fc-dbe4-4ae3-b2e3-d225849c0cc8",
   "metadata": {},
   "source": [
    "### Graded Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "842a7eac-08e9-4fbe-9b49-6eacf9ac0cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a1a to the activation function which provided the best validation accuracy at the epoch of minimum validation loss\n",
    "\n",
    "a1a = ...             # Replace with integer 0 (relu), 1 (sigmoid), or 2 (tanh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a9a0198-5fca-4534-bc32-038d1c9a77b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1a = Ellipsis\n"
     ]
    }
   ],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a1a = {a1a}') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d4178f3-2ec1-4d1f-a0e3-6404b85076d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a1b to the validation accuracy found by this best activation function\n",
    "\n",
    "a1b = 0.0             # Replace 0.0 with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0d6f803-0faa-4da0-aa22-4218552d4702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1b = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a1b = {a1b:.4f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4133730-e9a5-4b65-96a8-b59a5d7129ae",
   "metadata": {},
   "source": [
    "### Problem Two: Finding the Right Learning Rate\n",
    "\n",
    "In this problem, you will continue working with the **baseline model** and determine which learning rate produces the best performance. As before, the model you evaluate should be the one saved by **early stopping** — the epoch where validation loss is minimized.\n",
    "\n",
    "**Steps to follow:**\n",
    "\n",
    "* Build and train the **baseline model** using the **activation function identified in Problem One**.\n",
    "\n",
    "* Train and evaluate this model using each of the following learning rates:\n",
    "\n",
    "  ```\n",
    "      [1e-3, 5e-4, 1e-4, 5e-5, 1e-5]\n",
    "  ```\n",
    "\n",
    "* Identify which learning rate produces the **best validation accuracy** at the epoch of **minimum validation loss**, within a maximum of **500 epochs**.\n",
    "\n",
    "* Answer the graded questions.\n",
    "\n",
    "\n",
    "**Note: Smaller learning rates will generally take more epochs to reach the optimal point, so some of these will not engage early stopping, but run the full 500 epochs.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af5333b3-69f2-422b-9e36-112ad3c38d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Add as many cells as you need. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326704ab-721b-4ae0-8920-8c1ee64deb1c",
   "metadata": {},
   "source": [
    "#### Graded Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d8df2112-8ee8-47fd-91eb-5925479a709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a2a to the learning rate which provided the best validation accuracy at the epoch of minimum validation loss\n",
    "\n",
    "a2a = 0.0           # Replace 0.0 with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4a04f78-5620-41c8-b7ea-e1a2ae99b87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2a = 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a2a = {a2a:.6f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2abacc0-e2b9-49b0-8156-6b96b26571ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a2b to the validation accuracy found by this best learning rate\n",
    "\n",
    "a2b = 0.0             # Replace 0.0 with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4b7289fe-84e8-4fb4-bf74-83ebcfdfff3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a2b = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a2b = {a2b:.4f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145e838-d392-4044-b627-413645ce1077",
   "metadata": {},
   "source": [
    "### Problem Three: Dropout\n",
    "\n",
    "In this problem, you will explore how **dropout** can help prevent overfitting in neural networks. There are no absolute rules, but some useful hueristics are:\n",
    "\n",
    "* Dropout typically works best in **later dense layers** (e.g., the second hidden layer of width 32) in the range **0.3–0.5**.\n",
    "* If applied to **earlier layers** (e.g., the first hidden layer), dropout should be smaller, typically **0.0–0.2** (where 0.0 means no dropout).\n",
    "\n",
    "**Steps to follow:**\n",
    "\n",
    "* Build and train the **baseline model** using the **activation function from Problem One** and the **learning rate from Problem Two**.\n",
    "* Investigate dropout in the ranges suggested, using increments of **0.1**.\n",
    "* Identify which dropout configuration produces the **best validation accuracy** at the epoch of **minimum validation loss**.\n",
    "* Answer the graded questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "779f9f0d-bae4-4f5b-9212-cf72ad8f0331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Add as many cells as you need. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29a8ab51-f715-4547-a69e-38e581b612eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a3a to the pair (dropout_rate_64,dropout_rate_32) of dropout rates for the two hidden layers which provided the best \n",
    "# validation accuracy at the epoch of minimum validation loss\n",
    "\n",
    "a3a = (0.0,0.0)             # Replace (0.0,0.0) with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3966b26-5d5d-44e9-840b-9e2e54e3480e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3a = (0.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a3a = {a3a}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "76b6a2c7-7d04-4cbb-97b8-a6a319468509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a3b to the validation accuracy found by this best pair of dropout rates\n",
    "\n",
    "a3b = 0.0             # Replace 0.0 with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8426d026-e717-4ec0-8b70-ed29401f7b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a3b = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a3b = {a3b:.4f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f076b4-3fff-416e-ab3b-fd8c5ea65d47",
   "metadata": {},
   "source": [
    "### Problem Four: L2 Regularization\n",
    "\n",
    "In this problem, you will explore how **L2 regularization** (also called *weight decay*) can help prevent overfitting in neural networks. There are no absolute rules, but some useful heuristics are:\n",
    "\n",
    "* Start simple by using the **same λ in both hidden layers**, with values:\n",
    "\n",
    "  ```\n",
    "      1e-4, 1e-3, 1e-2\n",
    "  ```\n",
    "\n",
    "* If validation results suggest underfitting in the first layer or persistent overfitting in the later one, then try adjusting per layer, for example:\n",
    "\n",
    "  * First hidden layer: λ = 1e-4\n",
    "  * Second hidden layer: λ = 1e-3\n",
    "\n",
    "**Steps to follow:**\n",
    "\n",
    "* Build and train the **baseline model** using the **activation function from Problem One** and the **learning rate from Problem Two**, but **without dropout**.\n",
    "* Investigate at least the four cases suggested (three with the same λ and one with different λ values). You may also consider additional combinations.\n",
    "* Identify which configuration produces the **best validation accuracy** at the epoch of **minimum validation loss**.\n",
    "* Answer the graded questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28648ee2-a0e1-4778-a856-a4a3b388d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Add as many cells as you need. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "965f62fe-9bcd-4201-ab1e-6bd08e3694c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a4a to the pair (L2_lambda_64,L2_lambda_32) of the L2 lambdas for the two hidden layers which provided the best \n",
    "# validation accuracy at the epoch of minimum validation loss\n",
    "\n",
    "a4a = (0.0,0.0)             # Replace (0.0,0.0) with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "392fb8b5-1f21-4089-b742-fc8079ffd2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a4a = (0.0, 0.0)\n"
     ]
    }
   ],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a4a = {a4a}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc94054a-523b-4e82-b59c-f396c9567baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a4b to the validation accuracy found by this best pair of lambdas\n",
    "\n",
    "a4b = 0.0             # Replace 0.0 with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33c400cb-018b-4955-99f6-d827bbe8a838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a4b = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a4b = {a4b:.4f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d722c6-0f15-4329-becb-0a2694b3d9d1",
   "metadata": {},
   "source": [
    "### Problem Five: Combining Dropout with L2 Regularization\n",
    "\n",
    "In this problem, you will explore how **dropout** and **L2 regularization** can work together to prevent overfitting. These two methods complement each other, but must be balanced carefully. A useful rule of thumb is:\n",
    "\n",
    "* If dropout is **high**, use a **smaller λ**.\n",
    "* If dropout is **low**, you can afford a **larger λ**.\n",
    "\n",
    "**Steps to follow:**\n",
    "\n",
    "* Build and train the **baseline model** using the **activation function from Problem One** and the **learning rate from Problem Two**.\n",
    "* Investigate combinations of dropout and L2:\n",
    "\n",
    "  * First, use the **dropout rate you identified in Problem Three** as a baseline.\n",
    "  * Then, add L2 to both hidden layers with values:\n",
    "\n",
    "    ```\n",
    "        1e-4, 1e-3, 1e-2\n",
    "    ```\n",
    "\n",
    "    while keeping dropout fixed.\n",
    "  * Finally, try **reducing dropout slightly** when L2 is added to see if performance improves.\n",
    "  * [Optional] You may wish to investigate other combinations not covered here; for example, promising but not optimal choices of dropout rates may provide overall better performance when combines with L2 Regulari \n",
    "* Identify which combination produces the **best validation accuracy** at the epoch of **minimum validation loss**.\n",
    "* Answer the graded questions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4bf561b3-72e8-41a1-8d78-f045bd8a78f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Add as many cells as you need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "494220ff-038d-479b-a5e0-41ea22689f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a5 to the validation accuracy found by this best combination of dropout and L2 regularization\n",
    "\n",
    "a5 = 0.0             # Replace 0.0 with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4344ea31-ce34-4f22-8c32-b2d129bbe4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a5 = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a5 = {a5:.4f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7d6885-1b4e-4816-aa23-b21cc84559a0",
   "metadata": {},
   "source": [
    "### Problem Six: Build and Train Your Best Model\n",
    "\n",
    "In this final problem, you will design and train your **best-performing model** using the techniques explored in the previous problems. You may make your own choices for:\n",
    "\n",
    "* **Model architecture** (number of layers, widths, etc.)\n",
    "* **Learning rate**\n",
    "* **Batch size** (a new hyperparameter not varied in earlier problems)\n",
    "* **Dropout rates** in both layers\n",
    "* **L2 λ values** in both layers\n",
    "* **[Optional but strongly suggested]:** Learning rate scheduling, using either **Exponential Decay** or **Cosine Decay**.\n",
    "\n",
    "  * For Exponential Decay, typical decay rates are **0.90–0.999**, with **0.95** often a good starting point.\n",
    "\n",
    "**Steps to follow:**\n",
    "\n",
    "* Build and train the model according to your design choices.\n",
    "* Use early stopping as before to evaluate performance at the epoch of **minimum validation loss**.\n",
    "* Answer the graded question.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4cc1efeb-074d-4f5a-af9f-7e0e32a796cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6bba4617-49a2-44f7-a3bb-70dbc0022657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a6 to the validation accuracy found by this best model\n",
    "\n",
    "a6 = 0.0             # Replace 0.0 with your answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16703dde-cf87-4c94-94a5-72f66dc08d43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a6 = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Graded Answer\n",
    "# DO NOT change this cell in any way          \n",
    "\n",
    "print(f'a6 = {a6:.4f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d88df40-8183-4253-bf6b-ca97a8ddc1b5",
   "metadata": {},
   "source": [
    "### Optional: Print out your results of all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df39c1da-e5f7-442d-9903-5f65b3d45ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eabb07e-e976-4bee-8e6c-1839f67222d6",
   "metadata": {},
   "source": [
    "## Reflection Questions (ungraded)\n",
    "\n",
    "1. Activation Functions:\n",
    "\n",
    "    - Why do you think one activation function worked better than the others for this task?\n",
    "    \n",
    "    - How might this choice differ for deeper or wider networks?\n",
    "\n",
    "2. Learning Rate:\n",
    "\n",
    "    - Would a much smaller learning rate (with many more epochs) likely produce better accuracy?\n",
    "    \n",
    "    - When is it worth training longer with a smaller step size, and when is it unnecessary?\n",
    "\n",
    "3. Dropout vs. L2:\n",
    "\n",
    "    - Which form of regularization — dropout or L2 — gave better results in your experiments?\n",
    "    \n",
    "    - Why might one method be more effective in this setting?\n",
    "\n",
    "4. Combining Dropout and L2:\n",
    "\n",
    "    - Why might the combination of dropout and L2 sometimes perform worse than using one method alone?\n",
    "    \n",
    "    - What does this tell you about the balance between bias and variance in regularization?\n",
    "\n",
    "5. Best Model:\n",
    "\n",
    "    - When you designed your best model, what trade-offs did you notice between model complexity, training stability, and generalization?\n",
    "    \n",
    "    - Did learning rate scheduling (if you tried it) improve results? Why might it help?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
